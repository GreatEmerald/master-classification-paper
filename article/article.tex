% Minimalistic template for a generic article, geared towards MDPI guidelines
% Use PDFLaTeX
\documentclass[a4paper,10pt]{article}
% Packages provided by the MDPI template already
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{lineno}
\usepackage{microtype}
\usepackage{amsmath}
\usepackage{indentfirst}
\usepackage{booktabs} % For \toprule, etc.

% Custom packages
\usepackage[acronym,toc,shortcuts,nohypertypes=acronym]{glossaries}
\usepackage[colorlinks, allcolors=blue, unicode]{hyperref}
\usepackage{hyperxmp} % Import license metadata
\usepackage{titling} % Get the author for the metadata
\usepackage{textcomp} % \texttimes
%\usepackage[pdf]{graphviz} % Importing dot files; can be replaced with generated PDFs
\usepackage[norefs,nocites]{refcheck} % Develop: warn if figure is not referenced

% Track changes package, remove when final; using a drop-in rather than the old version in TeX Live
% Usage: \added, \deleted, \replaced{new}{old}, \highlight, \comment
% All followed by an optional [id=<your id>, comment=<some comment>]
\usepackage[xcolor={usenames,dvipsnames}]{changes}
\definecolor{gem}{HTML}{003300}
\definechangesauthor[color=gem]{dm}
\definechangesauthor[color=NavyBlue]{jv}
\definechangesauthor[color=Purple]{nt}
\definechangesauthor[color=Mahogany]{mh}

% My own definition for the sections that start with a bold name but don't appear in TOC
\newcommand{\minisection}[1]{\medskip \textbf{#1:}}


% Metadata
\title{Fractional land cover classification method assessment using PROBA-V satellite data}
\author{Dainius Masiliūnas, Nandin-Erdene Tsendbazar, Martin Herold, \\ Myroslava Lesiv, Jan Verbesselt}
% * <Dainius Masiliūnas> 11:20:08 26 Mar 2019 UTC+0100:
% TODO: fix newline issue

\hypersetup{
    pdflicenseurl={http://creativecommons.org/licenses/by-sa/4.0/},
    pdfcopyright={This work is licensed under the Creative Commons Attribution-ShareAlike 4.0 International License.},
    pdfauthor={\theauthor}, % These are supposed to be the default but don't seem to be
    pdftitle={\thetitle},
    pdflang={en-GB}
}

% Acronyms - cite with \ac{}
\newacronym{GNSS}{GNSS}{Global Navigation Satellite System}
\newacronym{OA}{OA}{Overall accuracy}
\newacronym{UA}{UA}{User accuracy}
\newacronym{PA}{PA}{Producer accuracy}
\newacronym{RMSE}{RMSE}{Root mean squared error}
\newacronym{MAE}{MAE}{Mean absolute error}
\newacronym{ME}{ME}{Mean error}
\newacronym{NSE}{NSE}{Nash–Sutcliffe model efficiency coefficient}
\makenoidxglossaries

% Use \begin{equation} for maths

\begin{document}

\maketitle

\linenumbers

\abstract{
The current common practice for producing global land cover maps is to represent land cover classes as discrete units, e.g. a pixel may be labelled as grassland, tree cover, or open water. However, in reality, the area covered by the pixel is hardly ever homogeneous, it may be e.g. trees surrounded by grass next to water. This mismatch between the reality and its representation in maps causes a loss in precision, as only the dominant land cover class can be represented in such ``hard'' classification maps. This problem is worse for coarse resolution land cover maps. A common workaround to this issue is to define mosaic classes, i.e. land cover classes that are combinations of other land cover classes, such as open forest (combination of grassland and tree cover). However, this approach leads to increasingly complex map legends and confusion about the definitions.

Our proposed solution to this issue is to employ fractional land cover mapping: for each pixel, define the proportion of the pixel that each considered land cover class occupies. This allows expressing an open forest as e.g. 40\% grassland, 60\% tree cover, and 0\% of the rest of the land cover classes. This ``fuzzy'' classification not only allows to more precisely express the land cover that exists in reality, but also gives users the flexibility to derive their own maps with a customised legend. For instance, if a user defines open forest as one that has less than 50\% tree cover and at least 30\% of grassland, they can generate a map with this definition from land cover fraction data, rather than relying on the definition that the mapmakers provide.
Exhaustive land cover fraction data is desired by a number of user communities, but to date there is no single global nor continental scale map that would provide fractions of each common land cover class.

In this paper, we tested the application of fractional land cover mapping over the continent of Africa. We compared the performance of four very different machine learning algorithms: random forest regression, multilayer perceptron neural networks, partial least squares regression, fuzzy nearest prototype classification and logistic regression. In order to allow the algorithms to make good predictions about land cover fractions, six types of covariates were used: spectral bands and vegetation indices derived from the Proba-V 100 m TOC reflectance data; temporal features derived from the full time series of said imagery; terrestrial features from a digital elevation model; soil properties and climate data from global models, and location. We compared the contribution of each of the covariate types to improving the accuracy of the model predictions.

One challenge when dealing with fractional data is the data imbalance: in most locations, only a single or a few classes dominate a pixel, with the rest taking up 0\% of the pixel. Values at the extremes (0\% and 100\%, i.e. pure pixels) are difficult for machine learning algorithms to estimate. We propose a new solution to this problem by combining three models: two classification models and one regression model. The first classification model determines whether a pixel is pure, and if so, it is processed using a classification model, otherwise it is processed with a regression model.

% Validation
% - 28k points
% - Independent training and validation sets

The models were trained on 26668 training points throughout Africa. The training points, collected as part of the Copernicus Global Land System: Dynamic Land Cover (CGLS-LC100) project, describe the fraction that each land cover class (bare soil, crops, grassland, shrubs, trees, urban/built-up, water and wetlands) occupies within a 100\texttimes100 m area, which is aligned to the grid used for Proba-V 100 m products. After the models were trained, their land cover class fraction predictions were compared with an independent validation set of 3617 points, which are likewise aligned to the Proba-V grid. Overall as well as per-class RMSE, MAE, and ME statistics were used to compare the models. In addition, the subpixel confusion-uncertainty matrix was used to analyse which classes were the most difficult to separate for each model, as well as to derive overall, users' and producers' accuracies and the kappa statistic. The models were compared with each other, as well as against a control intercept-only model.

Preliminary results show that the three-model approach increases the overall accuracy of the predictions by more accurately modelling the extreme values. From the machine learning models that were compared, random forest regression reaches the highest overall accuracy of 71\%. This is comparable to the reported accuracies of the current ``hard'' global land cover maps. Covariate importance analysis shows that spectral covariates are the most important for predicting land cover fractions, followed by the location, terrestrial and temporal covariates.

The application of the method on data gathered throughout Africa shows the applicability of the method to a variety of land cover conditions, ranging from highly heterogeneous agricultural landscapes to homogeneous bodies of water. Given enough reference data from other parts of the world and enough computational power, the method could be applied globally to upscale it to a global fractional land cover map. Such a map would cater to users with different fields of focus. Among other benefits, more precise estimation of land cover allows for 1) improved climate models to help reach Sustainable Development Goals (for which land cover is a key variable), 2) facilitate the making of models for vegetation and urban area dynamics, and 3) help governments and land owners keep better track of relevant land cover and its change. In addition, this work contributes to the operationalisation of the CGLS-LC100 project, which aims to produce exhaustive global land cover fractions as an operational product.

\minisection{Keywords} Fuzzy land cover classification, machine learning, random forest, gradient boosting, neural networks, fuzzy c-means, PROBA-V
}

\section{Introduction}

\begin{enumerate}
 \item Relevance of fractional LC mapping (link to user requirements and SDGs)
 \item Studies that have looked into fractional LC mapping so far
 \item Available fractional methods that were introduced in literature (wavelet transform etc.)
 \item Methods that are tested in this paper
\end{enumerate}

Global land cover maps 

\textbf{Objectives}:

\begin{enumerate}
 \item Compare the performance of a variety of machine learning models for a continental-scale fractional land cover mapping task
 \item Compare the importance of covariates and covariate groups for the prediction of land cover fractions
 \item Investigate methods for reducing bias in the predictions with regards to zero inflation and predictions tending towards the mean
\end{enumerate}


\section{Materials and Methods}

\subsection{Reference data}

Description of the CGLOPS data, mentioning the balance and zero inflation problems, and the fact that the study area is Africa

\subsection{Covariates/Features}

\begin{enumerate}
 \item GLSDEM
 \item Proba-V time series
 \item SoilGrids and LandGIS
 \item WorldClim
\end{enumerate}

\subsection{Preprocessing}

\begin{enumerate}
 \item Temporal cloud filter for Proba-V
 \item Harmonic metrics extraction
 \item Terrain metrics calculation
 \item Additional climate parameter calculation
 \item Manual model feature selection
\end{enumerate}

\begin{figure}
 %\inputdigraph[width=\textwidth]{article-figures/algorithms}{dot}
 \includegraphics[width=\textwidth]{article-figures/algorithms}
 \caption{Preprocessing chain.}
 \label{fig-preprocessing}
\end{figure}


\subsection{Fractional land cover mapping methods}

\subsubsection{Logistic regression}

\subsubsection{Lasso/ridge/elastic net regression}

\subsubsection{Partial least squares regression}

\subsubsection{Fuzzy nearest prototype/centroid}

\subsubsection{Neural networks}

AKA multilayer perceptron

\subsubsection{Random forest regression}

Including variable importance

\subsection{Model tuning (multi-step approach) to account for value imbalance}

As the dataset is unbalanced (dominated by zeroes and hundreds) and thus the objective function tends to draw the model towards good prediction of 0/100, whereas in regression the middle is also important. Therefore we tried several approaches to increase the accuracy of the middle predictions.

We compared a single model of the best algorithm, to a two-step approach: one model to predict zeroes, and one model to predict non-zeroes; and a three-step approach, with one model to predict pixel purity (i.e. if we face a classification or a regression problem), one to perform regression (on mixed pixels) and one to perform classification (on pure pixels).

In addition, we investigated applying histogram matching, by matching the histogram of the predictions back to the histogram of the training set.

\subsection{Validation/Accuracy Assessment}

Not sure if this should be a section, or just part of results:

\begin{enumerate}
 \item \ac{RMSE}, \ac{MAE}, \ac{ME} per class
 \item Sub-pixel confusion-uncertainty matrix: kappa, \ac{OA}, \ac{PA}, \ac{UA}
 \item Correlation matrix
 \item Prediction accuracy at different parts of the predicted vs observed line
 \item Comparison with a control/intercept model (10\% of everything)?
\end{enumerate}

\section{Results}

\begin{enumerate}
 \item RMSE per class comparison: no big differences
 \item Subpixel confusion matrix metrics: quite a big difference
 \item RF variable importance
 \item Truth:prediction scatterplots (hex/box/bar)
 \item Point map of Africa for the different classes
\end{enumerate}

\subsection{Method accuracy comparison}

The intercept model (always predicting the mean) achieves an \ac{RMSE} of 30\%, and \ac{OA}

Logistic regression achieved RMSE of 18.7\%, MAE of 9.8\%, 

\begin{table}
\centering
\begin{tabular}{llllll}
\toprule
\textbf{Model} & \textbf{\ac{RMSE} (\%)} & \textbf{\ac{MAE} (\%)} & \textbf{\acrshort{NSE}} & \textbf{\ac{OA} (\%)} & \textbf{Kappa} \\
\midrule
Intercept
& 30.7  & 21.8  & 0     & 24±4  & 0.11±0.04 \\
Fuzzy nearest centroid
& 22    &       &       & 49    & 0.41      \\
Neural networks
& 19.5  &       &       &       &           \\
PLS regression
& 19    &       &       & 47±5  & 0.36±0.07 \\
Logistic regression
& 20.0  & 10.7  & 0.53  & 66±4  & 0.57      \\
Lasso regression
& 18.5  &       &       &       &           \\
Random forest
& 16.6  & 9.2   & 0.67  & 68±4  & 0.60      \\
\ensuremath{''} histogram matched
& 22.0  & 9.7   & 0.43  & 66±2  & 0.58±0.03 \\
\ensuremath{''} two-step
& 18.3  & 8.0   & 0.59  & 72±2  & 0.63      \\
\ensuremath{''} three-step
& 18.6  & 8.2   & 0.59  & 71±4  & 0.64      \\
\ensuremath{''} \ensuremath{''} histogram matched
& 21.3  & 9.1   & 0.46  & 68±3  & 0.61      \\
\bottomrule
\end{tabular}
\caption{Accuracy statistics of the models tested.}
\label{tab-accuracy}
\end{table}

% 1:1 plots
\begin{figure}
    \includegraphics[width=0.8\textwidth]{article-figures/binplots/2019-04-12-rf-1m-uncor-points-a001s2}
    \caption{Random Forest single model prediction correspondence to ground truth (raw points with overplotting).}
    \label{raw-rf-1m-uncor}
\end{figure}
% Binplots
\begin{figure}
    \includegraphics[width=0.8\textwidth]{article-figures/binplots/2019-04-12-rf-1m-uncor-point-n100s20}
    \caption{Random Forest single model prediction correspondence to ground truth, 100x100 bins.}
    \label{bin-rf-1m-uncor-n100}
\end{figure}
\begin{figure}
    \includegraphics[width=0.8\textwidth]{article-figures/binplots/2019-04-12-rf-1m-uncor-point-n50s20sqrt}
    \caption{Random Forest single model prediction correspondence to ground truth, 50x50 bins, square root transformation.}
    \label{bin-rf-1m-uncor-n50sqrt}
\end{figure}
\begin{figure}
    \includegraphics[width=0.8\textwidth]{article-figures/binplots/2019-04-12-rf-1m-uncor-point-n50s10sqrt}
    \caption{Random Forest single model prediction correspondence to ground truth, 50x50 bins, square root transformation, smaller circle sizes.}
    \label{bin-rf-1m-uncor-n50sqrts10}
\end{figure}
\begin{figure}
    \includegraphics[width=0.8\textwidth]{article-figures/binplots/2019-04-12-rf-1m-uncor-raster-n50sqrt}
    \caption{Random Forest single model prediction correspondence to ground truth, square root transformation.}
    \label{bin-rf-1m-uncor-rastersqrt}
\end{figure}

% Hexplots
\begin{figure}
    \includegraphics[width=0.8\textwidth]{article-figures/hexplots/2019-03-22-rf-1m-uncor-hex}
    \caption{Random Forest single model prediction correspondence to ground truth.}
    \label{hex-rf-1m-uncor}
\end{figure}
\begin{figure}
    \includegraphics[width=0.8\textwidth]{article-figures/hexplots/2019-03-22-rf-3m-uncor-hex}
    \caption{Random Forest three-step model prediction correspondence to ground truth.}
    \label{hex-rf-3m-uncor}
\end{figure}
\begin{figure}
    \includegraphics[width=0.8\textwidth]{article-figures/hexplots/2019-03-22-rf-1m-uncor-hm-hex}
    \caption{Random Forest single model prediction (histogram matched) correspondence to ground truth.}
    \label{hex-rf-1m-uncor-hm}
\end{figure}
\begin{figure}
    \includegraphics[width=0.8\textwidth]{article-figures/hexplots/2019-03-22-rf-3m-uncor-hm-hex}
    \caption{Random Forest three-step model prediction (histogram matched) correspondence to ground truth.}
    \label{hex-rf-3m-uncor-hm}
\end{figure}
\begin{figure}
    \includegraphics[width=0.8\textwidth]{article-figures/hexplots/2019-03-22-rf-3m-uncor-hma-hex}
    \caption{Random Forest three-step model prediction (histogram matched middle, proportionally) correspondence to ground truth.}
    \label{hex-rf-3m-uncor-hma}
\end{figure}
\begin{figure}
    \includegraphics[width=0.8\textwidth]{article-figures/hexplots/2019-03-22-rf-3m-uncor-hmb-hex}
    \caption{Random Forest three-step model prediction (histogram matched non-extremes) correspondence to ground truth.}
    \label{hex-rf-3m-uncor-hmb}
\end{figure}

% Boxplots
\begin{figure}
    \includegraphics[width=0.8\textwidth]{article-figures/boxplots/2019-03-19-rf-1m-uncor-box}
    \caption{Random Forest single model prediction correspondence to ground truth.}
    \label{box-rf-1m-uncor-hm}
\end{figure}
\begin{figure}
    \includegraphics[width=0.8\textwidth]{article-figures/boxplots/2019-03-19-rf-2m-uncor-box}
    \caption{Random Forest two-step model prediction correspondence to ground truth.}
    \label{box-rf-2m-uncor-hm}
\end{figure}
\begin{figure}
    \includegraphics[width=0.8\textwidth]{article-figures/boxplots/2019-03-19-rf-3m-uncor-box}
    \caption{Random Forest three-step model prediction correspondence to ground truth.}
    \label{box-rf-3m-uncor-hm}
\end{figure}

% Barplots
\begin{figure}
    \includegraphics[width=0.8\textwidth]{article-figures/barplots/2019-03-19-rf-1m-uncor-bar}
    \caption{Random Forest single model prediction accuracies per class, per predicted magnitude (extremes vs middle)}
    \label{bar-rf-1m-uncor}
\end{figure}
\begin{figure}
    \includegraphics[width=0.8\textwidth]{article-figures/barplots/2019-03-19-rf-2m-uncor-bar}
    \caption{Random Forest two-step model prediction accuracies per class, per predicted magnitude (extremes vs middle)}
    \label{bar-rf-2m-uncor}
\end{figure}
\begin{figure}
    \includegraphics[width=0.8\textwidth]{article-figures/barplots/2019-03-19-rf-3m-uncor-bar}
    \caption{Random Forest three-step model prediction accuracies per class, per predicted magnitude (extremes vs middle)}
    \label{bar-rf-3m-uncor}
\end{figure}
\begin{figure}
    \includegraphics[width=0.8\textwidth]{article-figures/barplots/2019-03-22-rf-1m-uncor-hm-bar}
    \caption{Random Forest single model prediction (histogram matched) accuracies per class, per predicted magnitude (extremes vs middle)}
    \label{bar-rf-1m-uncor-hm}
\end{figure}
\begin{figure}
    \includegraphics[width=0.8\textwidth]{article-figures/barplots/2019-03-22-rf-3m-uncor-hm-bar}
    \caption{Random Forest three-step model prediction (histogram matched) accuracies per class, per predicted magnitude (extremes vs middle)}
    \label{bar-rf-3m-uncor-hm}
\end{figure}

% Maps
\begin{figure}
    \includegraphics[width=\textwidth]{article-figures/maps/2019-03-26-rf-1m-bubble}
    \caption{Random Forest single model prediction residuals per class, spatially}
    \label{resid-rf-1m-uncor}
\end{figure}
\begin{figure}
    \includegraphics[width=\textwidth]{article-figures/maps/2019-03-26-rf-3m-bubble}
    \caption{Random Forest three-step model prediction residuals per class, spatially}
    \label{resid-rf-3m-uncor}
\end{figure}
\begin{figure}
    \includegraphics[width=\textwidth]{article-figures/maps/2019-04-10-rasterised-validation}
    \caption{Validation data spatially}
    \label{raster-validation}
\end{figure}
\begin{figure}
    \includegraphics[width=\textwidth]{article-figures/maps/2019-04-10-rasterised-rf-1m-uncor}
    \caption{Random Forest single model predictions spatially}
    \label{raster-rf-1m-uncor}
\end{figure}

\section{Discussion}

\begin{enumerate}
 \item Fraction area accuracy goes up to 72\%, that is good considering that hard classification is not much better than that
 \item RF is the best, even though one model per class means that it does not take everything into account
 \item Accuracy assessment metric makes a big difference, useful to use subpixel confusion matrix for that
 \item Hardest to classify are grasslands and especially shrubs and urban
 \item All covariates are important to a certain degree (show table of importances)
 \item Histogram matching doesn't help any
\end{enumerate}

\section{Conclusions}

\begin{enumerate}
 \item Random forest is the best by some margin
 \item Subpixel confusion matrix is useful for differentiation
 \item All covariates are important but to a different degree
 \item Using a multi-model approach improves MAE at the cost of RMSE, thus a trade-off
\end{enumerate}

\minisection{Author Contributions} DM JV NT MH ML BK

\minisection{Funding} JRC CGLOPS

\minisection{Acknowledgements} VITO, time series group (BB)

\minisection{Conflicts of Interest} The authors declare no conflict of interest.

%\section*{Abbreviations}

\printnoidxglossary[type=acronym]

\end{document}
